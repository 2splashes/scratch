{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading the lending club loans dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (19,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "loans = pd.read_csv('lending-club-data.csv')\n",
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "loans = loans.drop('bad_loans', axis=1)\n",
    "\n",
    "with open('module-5-assignment-2-train-idx.json','r') as f:\n",
    "    train_idx_list = json.load(f)\n",
    "train_idx = [int(i) for i in train_idx_list]\n",
    "\n",
    "with open('module-5-assignment-2-test-idx.json', 'r') as f:\n",
    "    test_idx_list = json.load(f)\n",
    "test_idx = [int(i) for i in test_idx_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home_ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "target = 'safe_loans'\n",
    "\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "loans_data = pd.get_dummies(loans[features], prefix=features, prefix_sep='.')\n",
    "loans_data = pd.concat([loans_data, loans[target]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loans_data.columns[:-1]) # number of features (i.e. excl. target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_features = list(loans_data.columns[:-1])\n",
    "train_data = loans_data.iloc[train_idx]\n",
    "test_data = loans_data.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['grade.A', 'grade.B', 'grade.C', 'grade.D', 'grade.E', 'grade.F',\n",
       "       'grade.G', 'term. 36 months', 'term. 60 months',\n",
       "       'home_ownership.MORTGAGE', 'home_ownership.OTHER', 'home_ownership.OWN',\n",
       "       'home_ownership.RENT', 'emp_length.1 year', 'emp_length.10+ years',\n",
       "       'emp_length.2 years', 'emp_length.3 years', 'emp_length.4 years',\n",
       "       'emp_length.5 years', 'emp_length.6 years', 'emp_length.7 years',\n",
       "       'emp_length.8 years', 'emp_length.9 years', 'emp_length.< 1 year',\n",
       "       'emp_length.n/a', 'safe_loans'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grade.A',\n",
       " 'grade.B',\n",
       " 'grade.C',\n",
       " 'grade.D',\n",
       " 'grade.E',\n",
       " 'grade.F',\n",
       " 'grade.G',\n",
       " 'term. 36 months',\n",
       " 'term. 60 months',\n",
       " 'home_ownership.MORTGAGE',\n",
       " 'home_ownership.OTHER',\n",
       " 'home_ownership.OWN',\n",
       " 'home_ownership.RENT',\n",
       " 'emp_length.1 year',\n",
       " 'emp_length.10+ years',\n",
       " 'emp_length.2 years',\n",
       " 'emp_length.3 years',\n",
       " 'emp_length.4 years',\n",
       " 'emp_length.5 years',\n",
       " 'emp_length.6 years',\n",
       " 'emp_length.7 years',\n",
       " 'emp_length.8 years',\n",
       " 'emp_length.9 years',\n",
       " 'emp_length.< 1 year',\n",
       " 'emp_length.n/a']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementing a binary decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Btree_Model(object):\n",
    "    '''\n",
    "    An implementation of binary decision trees. Works only for binary features.\n",
    "    An instance of the class is a model - in order to create the tree, use the create method, which returns the tree as a recursive dictionary. Hence, the instance (model) can be used to create several trees.\n",
    "    Note that only the last tree is stored as an attribute.\n",
    "    \n",
    "    The model is equipped with classification and node count methods, but a tree must be passed to each.\n",
    "    '''\n",
    "    def __init__(self, max_depth=10, min_node_size=1, \n",
    "                 min_error_reduction=0.0):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "        self.min_node_size = min_node_size\n",
    "        self.min_error_reduction = min_error_reduction\n",
    "        print('A binary decision tree of max depth %d is initiated.' \\\n",
    "              %(self.max_depth))\n",
    "        print('No further splits are allowed if a node has fewer than %d data points' \\\n",
    "             %self.min_node_size)\n",
    "        print('As a stopping condition, we require that each split generate error reduction of at least %.1f' \\\n",
    "             %(self.min_error_reduction))\n",
    "    \n",
    "    @staticmethod\n",
    "    def intermediate_node_num_mistakes(labels_in_node):\n",
    "        if len(labels_in_node) == 0: return 0  \n",
    "        labels_in_node = np.array(labels_in_node)\n",
    "        num_positive = np.sum(labels_in_node==1)\n",
    "        num_negative = np.sum(labels_in_node==-1)\n",
    "        \n",
    "        # Classification is by the majority rule, hence the minority class encompasses all mistakes\n",
    "        num_mistakes = min(num_positive, num_negative)\n",
    "        return num_mistakes\n",
    "    \n",
    "    @staticmethod\n",
    "    def best_splitting_feature(data, features, target):\n",
    "        assert type(features)==list and type(target)==str, \"Features must be a list, and target must be a string.\"\n",
    "        target_values = data[target]\n",
    "        best_feature = None # Keep track of the best feature \n",
    "        best_error = 10     # Keep track of the best error so far\n",
    "        \n",
    "        # num_data_points = float(len(data)) # float division not an issue in Python 3\n",
    "        \n",
    "        for feature in features:\n",
    "            # 1. Split on the binary feature\n",
    "            left_split = data[data[feature] == 0]\n",
    "            right_split = data[data[feature] == 1]\n",
    "            \n",
    "            # 2. Calculate the number of misclassified examples\n",
    "            left_mistakes = Btree_Model.intermediate_node_num_mistakes(left_split[target])\n",
    "            right_mistakes = Btree_Model.intermediate_node_num_mistakes(right_split[target])\n",
    "            \n",
    "            # 3. Calculate classification error for the split on this feature\n",
    "            error = (left_mistakes + right_mistakes) / data.shape[0]\n",
    "            \n",
    "            # 4. Track the best feature\n",
    "            if error <= best_error:\n",
    "                best_feature = feature\n",
    "                best_error = error\n",
    "        return best_feature\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_leaf(target_values):\n",
    "        '''\n",
    "        Each node in the decision tree is represented as a dictionary.\n",
    "        '''\n",
    "        leaf = {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True}\n",
    "        \n",
    "        num_ones = len(target_values[target_values == +1])\n",
    "        num_minus_ones = len(target_values[target_values == -1])\n",
    "        \n",
    "        if num_ones > num_minus_ones:\n",
    "            leaf['prediction'] = +1\n",
    "        else:\n",
    "            leaf['prediction'] = -1\n",
    "        \n",
    "        return leaf\n",
    "    \n",
    "    @staticmethod\n",
    "    def error_reduction(error_before_split, error_after_split):\n",
    "        '''\n",
    "        computes the gain in error following a node split.\n",
    "        '''\n",
    "        return error_before_split - error_after_split\n",
    "    \n",
    "    def reached_minimum_node_size(self, data):\n",
    "        return len(data) <= self.min_node_size\n",
    "    \n",
    "    def create(self, data, features, target, current_depth = 0):\n",
    "        '''\n",
    "        A recursive greedy algorithm to build the binary decision tree. \n",
    "        Max depth is stored as an attribute of the instance.\n",
    "        '''\n",
    "        self.target = target\n",
    "        self.features = features\n",
    "        remaining_features = list(features)\n",
    "        target_values = data[target]\n",
    "        print(\"--------------------------------------------------------------------\")\n",
    "        print(\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values)))\n",
    "        \n",
    "        # A recursive algorithm, hence we define stopping conditions\n",
    "        \n",
    "        if Btree_Model.intermediate_node_num_mistakes(target_values) == 0:\n",
    "            print(\"Stopping condition 1 (single class node) reached.\")     \n",
    "            # If no mistakes at current node, make current node a leaf node\n",
    "            return Btree_Model.create_leaf(target_values)\n",
    "        \n",
    "        if len(remaining_features) == 0:\n",
    "            print(\"Stopping condition 2 (no more features) reached.\")     \n",
    "            return Btree_Model.create_leaf(target_values)\n",
    "        \n",
    "        if current_depth >= self.max_depth:\n",
    "            print(\"Stopping condition 3 (max depth) reached.\")     \n",
    "            return Btree_Model.create_leaf(target_values)\n",
    "        \n",
    "        if self.reached_minimum_node_size(target_values):       \n",
    "            print(\"Early stopping condition reached. Reached minimum node size.\")\n",
    "            return Btree_Model.create_leaf(target_values)\n",
    "        \n",
    "        # Determine best feature to split on\n",
    "        splitting_feature = Btree_Model.best_splitting_feature(data, \n",
    "                                        remaining_features, target)\n",
    "        \n",
    "        # Split the node\n",
    "        left_split = data[data[splitting_feature] == 0]\n",
    "        right_split = data[data[splitting_feature] == 1]\n",
    "        remaining_features.remove(splitting_feature)\n",
    "        print(\"Split on feature %s. (%d, %d)\" % (\\\n",
    "                      splitting_feature, len(left_split), len(right_split)))\n",
    "        \n",
    "        # Another stopping condition: don't split if insufficient error reduction\n",
    "        error_before_split = Btree_Model.intermediate_node_num_mistakes(target_values) / len(data)\n",
    "    \n",
    "        left_mistakes = Btree_Model.intermediate_node_num_mistakes(left_split[target])\n",
    "        right_mistakes = Btree_Model.intermediate_node_num_mistakes(right_split[target])\n",
    "        error_after_split = (left_mistakes + right_mistakes) / len(data)\n",
    "    \n",
    "        if Btree_Model.error_reduction(error_before_split, \n",
    "                                       error_after_split) <= self.min_error_reduction: \n",
    "            print(\"Early stopping condition reached. Minimum error reduction.\")\n",
    "            return Btree_Model.create_leaf(target_values)\n",
    "        \n",
    "        # If a split generates an empty node (i.e., all data goes to either left or right)\n",
    "        if len(left_split) == len(data):\n",
    "            print(\"Creating leaf node.\")\n",
    "            return Btree_Model.create_leaf(left_split[target])\n",
    "        if len(right_split) == len(data):\n",
    "            print(\"Creating leaf node.\")\n",
    "            return Btree_Model.create_leaf(right_split[target])\n",
    "        \n",
    "        left_tree = self.create(left_split, remaining_features, \n",
    "                                target, current_depth+1)\n",
    "        right_tree = self.create(right_split, remaining_features, \n",
    "                                 target, current_depth+1)\n",
    "        \n",
    "        self.tree = {'is_leaf'          : False, \n",
    "                    'prediction'       : None,\n",
    "                    'splitting_feature': splitting_feature,\n",
    "                    'left'             : left_tree, \n",
    "                    'right'            : right_tree}\n",
    "        \n",
    "        \n",
    "        return self.tree\n",
    "    \n",
    "    def classify(self, tree, obs, annotate=False):\n",
    "        '''\n",
    "        Classifies an observation (array-like of inputs) by traversing the tree.\n",
    "        '''\n",
    "        if tree['is_leaf']:\n",
    "            if annotate:\n",
    "                print(\"At leaf, predicting %s\" %(tree['prediction']))\n",
    "            return tree['prediction']\n",
    "        else:\n",
    "            split_feature_value = obs[tree['splitting_feature']]\n",
    "            if annotate:\n",
    "                print(\"Split on %s = %s\" % (tree['splitting_feature'], \n",
    "                                            split_feature_value))\n",
    "            if split_feature_value == 0:\n",
    "                return self.classify(tree['left'], obs, annotate)\n",
    "            else:\n",
    "                return self.classify(tree['right'], obs, annotate)\n",
    "    \n",
    "    def count_nodes(self, tree):\n",
    "        if tree['is_leaf']:\n",
    "            return 1\n",
    "        return 1 + self.count_nodes(tree['left']) + self.count_nodes(tree['right'])\n",
    "    \n",
    "    def evaluate(self, tree, data):\n",
    "        # calcuates classification error on data\n",
    "        predictions = data.apply(lambda x: self.classify(tree, x), axis=1)\n",
    "        accuracy = np.sum(predictions==data[self.target])/data.shape[0]\n",
    "        return 1-accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A binary decision tree of max depth 6 is initiated.\n",
      "No further splits are allowed if a node has fewer than 100 data points\n",
      "As a stopping condition, we require that each split generate error reduction of at least 0.0\n"
     ]
    }
   ],
   "source": [
    "binary_dtree_model_new = Btree_Model(max_depth=6, min_node_size = 100, min_error_reduction=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (37224 data points).\n",
      "Split on feature term. 60 months. (28001, 9223)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (28001 data points).\n",
      "Split on feature grade.D. (23300, 4701)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23300 data points).\n",
      "Split on feature grade.E. (22024, 1276)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (22024 data points).\n",
      "Split on feature grade.F. (21666, 358)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (21666 data points).\n",
      "Split on feature emp_length.n/a. (20734, 932)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (20734 data points).\n",
      "Split on feature grade.G. (20638, 96)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (20638 data points).\n",
      "Stopping condition 3 (max depth) reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (96 data points).\n",
      "Stopping condition 3 (max depth) reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (932 data points).\n",
      "Split on feature grade.A. (702, 230)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (702 data points).\n",
      "Stopping condition 3 (max depth) reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (230 data points).\n",
      "Stopping condition 3 (max depth) reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (358 data points).\n",
      "Split on feature emp_length.8 years. (347, 11)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (347 data points).\n",
      "Split on feature emp_length.n/a. (330, 17)\n",
      "Early stopping condition reached. Minimum error reduction.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Early stopping condition reached. Reached minimum node size.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (1276 data points).\n",
      "Split on feature emp_length.n/a. (1215, 61)\n",
      "Early stopping condition reached. Minimum error reduction.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (4701 data points).\n",
      "Split on feature emp_length.n/a. (4494, 207)\n",
      "Early stopping condition reached. Minimum error reduction.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (9223 data points).\n",
      "Split on feature grade.A. (9122, 101)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9122 data points).\n",
      "Split on feature emp_length.n/a. (8901, 221)\n",
      "Early stopping condition reached. Minimum error reduction.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (101 data points).\n",
      "Split on feature emp_length.n/a. (96, 5)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (96 data points).\n",
      "Early stopping condition reached. Reached minimum node size.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (5 data points).\n",
      "Early stopping condition reached. Reached minimum node size.\n"
     ]
    }
   ],
   "source": [
    "my_decision_tree_new = binary_dtree_model_new.create(train_data, binary_features, 'safe_loans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "Split on term. 60 months = 1.0\n",
      "Split on grade.A = 0.0\n",
      "At leaf, predicting -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_data.iloc[0][target])\n",
    "binary_dtree_model_new.classify(my_decision_tree_new, test_data.iloc[0], annotate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stump(tree, name = 'root'):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print(\"(leaf, label: %s)\" % tree['prediction'])\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('.')\n",
    "    print( '                       %s' % name)\n",
    "    print( '         |---------------|----------------|')\n",
    "    print( '         |                                |')\n",
    "    print( '         |                                |')\n",
    "    print( '         |                                |')\n",
    "    print( '  [{0} == 0]               [{0} == 1]    '.format(split_name))\n",
    "    print( '         |                                |')\n",
    "    print( '         |                                |')\n",
    "    print( '         |                                |')\n",
    "    print( '    (%s)                         (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluating classification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38367083153813009"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_dtree_model_new.evaluate(my_decision_tree_new, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_dtree_model_new.count_nodes(my_decision_tree_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
